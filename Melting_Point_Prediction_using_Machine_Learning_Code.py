# -*- coding: utf-8 -*-
"""Untitled48.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nIEBvVYwFAleV3m3oEAD0HflDAwqfR0V
"""

"""
Melting Point Prediction Pipeline
==================================

This script implements a complete machine learning pipeline for predicting molecular melting points.
The pipeline includes data preprocessing, feature engineering, model training, hyperparameter tuning,
and ensemble methods.

Author: [Your Name]
Date: 2025
License: MIT
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from sklearn.model_selection import train_test_split, cross_validate, RandomizedSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.impute import SimpleImputer
from sklearn.feature_selection import VarianceThreshold, mutual_info_regression
from sklearn.linear_model import Ridge, Lasso, ElasticNet
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import (RandomForestRegressor, AdaBoostRegressor,
                              GradientBoostingRegressor, StackingRegressor, VotingRegressor)
from sklearn.svm import SVR
from xgboost import XGBRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import warnings
warnings.filterwarnings('ignore')

# Set plotting style for better visualizations
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")

# Random seed for reproducibility - important for comparing different runs
RANDOM_STATE = 42
np.random.seed(RANDOM_STATE)


class MeltingPointPipeline:
    """
    A comprehensive pipeline for melting point prediction using machine learning.

    This class handles the entire workflow from data loading to final predictions,
    including parallel evaluation of original vs PCA-transformed features.
    """

    def __init__(self):
        """Initialize the pipeline with necessary transformers and containers."""
        # Preprocessing transformers
        self.scaler = StandardScaler()
        self.pca = PCA(n_components=0.95, random_state=RANDOM_STATE)  # Keep 95% variance
        self.imputer = SimpleImputer(strategy='mean')
        self.variance_selector = VarianceThreshold(threshold=0.01)

        # Storage for features and results
        self.selected_features = None
        self.results = {}
        self.results_log = []  # Will store all output for saving to file

    def log_result(self, text):
        """
        Print and store output for later saving to file.
        This ensures we can review all results even after the script finishes.
        """
        print(text)
        self.results_log.append(text)

    def save_results_to_file(self):
        """Save the complete execution log to a text file."""
        with open('model_results.txt', 'w') as f:
            f.write('\n'.join(self.results_log))
        print("\n‚úì All results saved to 'model_results.txt'")

    def load_data(self, train_path='train.csv', test_path='test.csv'):
        """
        Load training and test datasets.

        Args:
            train_path: Path to training CSV file
            test_path: Path to test CSV file

        Returns:
            self: For method chaining
        """
        header = "=" * 80
        self.log_result(header)
        self.log_result("MELTING POINT PREDICTION PIPELINE")
        self.log_result("MODEL SELECTION BASED ON MAE (Mean Absolute Error)")
        self.log_result(header)
        self.log_result("\n" + "=" * 80)
        self.log_result("STEP 1: LOADING DATA")
        self.log_result("=" * 80)

        self.train_df = pd.read_csv(train_path)
        self.test_df = pd.read_csv(test_path)

        self.log_result(f"Training data shape: {self.train_df.shape}")
        self.log_result(f"Test data shape: {self.test_df.shape}")
        self.log_result(f"\nTarget variable (Tm) statistics:")
        self.log_result(str(self.train_df['Tm'].describe()))

        return self

    def exploratory_analysis(self):
        """
        Perform exploratory data analysis with visualizations.
        Checks for missing values, outliers, and distribution characteristics.
        """
        self.log_result("\n" + "=" * 80)
        self.log_result("STEP 2: EXPLORATORY DATA ANALYSIS")
        self.log_result("=" * 80)

        # Check for missing values - important to know before imputation
        missing = self.train_df.isnull().sum()
        missing_pct = 100 * missing / len(self.train_df)
        missing_table = pd.DataFrame({
            'Missing': missing[missing > 0],
            'Percentage': missing_pct[missing > 0]
        }).sort_values('Percentage', ascending=False)

        if len(missing_table) > 0:
            self.log_result(f"\nMissing values detected in {len(missing_table)} columns:")
            self.log_result(str(missing_table.head(10)))
        else:
            self.log_result("\nNo missing values detected!")

        # Analyze target variable distribution
        self.log_result(f"\nTarget (Tm) distribution:")
        self.log_result(f"  Mean: {self.train_df['Tm'].mean():.2f}")
        self.log_result(f"  Median: {self.train_df['Tm'].median():.2f}")
        self.log_result(f"  Std: {self.train_df['Tm'].std():.2f}")
        self.log_result(f"  Skewness: {self.train_df['Tm'].skew():.2f}")

        # Detect outliers using the IQR method (1.5 * IQR rule)
        Q1 = self.train_df['Tm'].quantile(0.25)
        Q3 = self.train_df['Tm'].quantile(0.75)
        IQR = Q3 - Q1
        outliers = ((self.train_df['Tm'] < (Q1 - 1.5 * IQR)) |
                   (self.train_df['Tm'] > (Q3 + 1.5 * IQR))).sum()
        self.log_result(f"  Outliers (IQR method): {outliers} ({100*outliers/len(self.train_df):.2f}%)")

        # Feature summary
        feature_cols = [col for col in self.train_df.columns if col.startswith('Group')]
        self.log_result(f"\nFeature columns: {len(feature_cols)}")
        self.log_result(f"Zero variance features: {(self.train_df[feature_cols].std() == 0).sum()}")

        # Create visualization of target distribution
        fig, axes = plt.subplots(1, 2, figsize=(14, 5))

        # Histogram shows overall distribution
        axes[0].hist(self.train_df['Tm'], bins=50, edgecolor='black', alpha=0.7)
        axes[0].set_xlabel('Melting Point (Tm) [K]', fontsize=15, fontweight='bold')
        axes[0].set_ylabel('Frequency', fontsize=15, fontweight='bold')
        axes[0].set_title('Distribution of Melting Points', fontsize=16, fontweight='bold')
        axes[0].tick_params(axis='both', labelsize=15)
        axes[0].grid(True, alpha=0.3)

        # Box plot helps identify outliers visually
        axes[1].boxplot(self.train_df['Tm'], vert=True)
        axes[1].set_ylabel('Melting Point (Tm) [K]', fontsize=15, fontweight='bold')
        axes[1].set_title('Box Plot - Outlier Detection', fontsize=16, fontweight='bold')
        axes[1].tick_params(axis='both', labelsize=15)
        axes[1].grid(True, alpha=0.3)

        plt.tight_layout()
        plt.savefig('01_target_distribution.png', dpi=300, bbox_inches='tight')
        plt.close()
        self.log_result("\n‚úì Saved: 01_target_distribution.png")

        return self

    def prepare_data(self):
        """
        Prepare data by splitting into training and validation sets.
        We do this BEFORE preprocessing to prevent data leakage.
        """
        self.log_result("\n" + "=" * 80)
        self.log_result("STEP 3: DATA PREPARATION")
        self.log_result("=" * 80)

        # Extract only the Group features (molecular descriptors)
        feature_cols = [col for col in self.train_df.columns if col.startswith('Group')]

        X = self.train_df[feature_cols].values
        y = self.train_df['Tm'].values
        self.test_ids = self.test_df['id'].values
        X_test = self.test_df[feature_cols].values

        # 80/20 split - standard practice for model evaluation
        self.X_train, self.X_val, self.y_train, self.y_val = train_test_split(
            X, y, test_size=0.2, random_state=RANDOM_STATE
        )
        self.X_test_original = X_test

        self.log_result(f"Training set: {self.X_train.shape}")
        self.log_result(f"Validation set: {self.X_val.shape}")
        self.log_result(f"Test set: {X_test.shape}")

        return self

    def impute_missing(self):
        """
        Fill in missing values using mean imputation.
        We fit only on training data to prevent leakage.
        """
        self.log_result("\n" + "=" * 80)
        self.log_result("STEP 4: MISSING VALUE IMPUTATION")
        self.log_result("=" * 80)

        # Learn imputation strategy from training data only
        self.X_train = self.imputer.fit_transform(self.X_train)
        # Apply the same strategy to validation and test
        self.X_val = self.imputer.transform(self.X_val)
        self.X_test_original = self.imputer.transform(self.X_test_original)

        self.log_result("Imputation completed using mean strategy")

        return self

    def feature_selection(self):
        """
        Remove uninformative features using two methods:
        1. Variance threshold - removes features with very low variance
        2. Mutual information - keeps features that correlate with target
        """
        self.log_result("\n" + "=" * 80)
        self.log_result("STEP 5: FEATURE SELECTION")
        self.log_result("=" * 80)

        # First, remove features with almost no variation (they won't help prediction)
        self.X_train = self.variance_selector.fit_transform(self.X_train)
        self.X_val = self.variance_selector.transform(self.X_val)
        self.X_test_original = self.variance_selector.transform(self.X_test_original)

        n_remaining = self.X_train.shape[1]
        self.log_result(f"After variance threshold (0.01): {n_remaining} features remaining")

        # Calculate mutual information - measures dependency between features and target
        # Higher MI means the feature is more informative about the target
        mi_scores = mutual_info_regression(
            self.X_train, self.y_train,
            random_state=RANDOM_STATE,
            n_neighbors=5
        )

        # Keep top 75% of features by MI score
        mi_threshold = np.percentile(mi_scores, 25)
        self.selected_features = mi_scores > mi_threshold

        self.X_train = self.X_train[:, self.selected_features]
        self.X_val = self.X_val[:, self.selected_features]
        self.X_test_original = self.X_test_original[:, self.selected_features]

        self.log_result(f"After mutual information selection: {self.X_train.shape[1]} features remaining")
        self.log_result(f"Total features removed: {424 - self.X_train.shape[1]}")

        # Visualize which features are most important
        fig, ax = plt.subplots(figsize=(12, 6))
        sorted_idx = np.argsort(mi_scores)[-20:]  # Show top 20
        ax.barh(range(len(sorted_idx)), mi_scores[sorted_idx])
        ax.set_yticks(range(len(sorted_idx)))
        ax.set_yticklabels([f'Feature {i}' for i in sorted_idx], fontsize=12)
        ax.set_xlabel('Mutual Information Score', fontsize=15, fontweight='bold')
        ax.set_ylabel('Features', fontsize=15, fontweight='bold')
        ax.set_title('Top 20 Most Informative Features', fontsize=16, fontweight='bold')
        ax.tick_params(axis='x', labelsize=15)
        ax.grid(True, alpha=0.3, axis='x')
        plt.tight_layout()
        plt.savefig('02_feature_importance.png', dpi=300, bbox_inches='tight')
        plt.close()
        self.log_result("\n‚úì Saved: 02_feature_importance.png")

        return self

    def scale_features(self):
        """
        Standardize features to have mean=0 and std=1.
        This is crucial for algorithms like Ridge, Lasso, and SVR.
        """
        self.log_result("\n" + "=" * 80)
        self.log_result("STEP 6: FEATURE SCALING")
        self.log_result("=" * 80)

        # Learn scaling parameters from training data
        self.X_train_scaled = self.scaler.fit_transform(self.X_train)
        # Apply same scaling to validation and test
        self.X_val_scaled = self.scaler.transform(self.X_val)
        self.X_test_scaled = self.scaler.transform(self.X_test_original)

        self.log_result("Features scaled using StandardScaler")
        self.log_result(f"Mean: {self.X_train_scaled.mean():.4f}, Std: {self.X_train_scaled.std():.4f}")

        return self

    def create_pca_features(self):
        """
        Apply PCA to create an alternative feature set.
        PCA can help by:
        - Reducing dimensionality
        - Removing multicollinearity
        - Capturing main patterns in data
        """
        self.log_result("\n" + "=" * 80)
        self.log_result("STEP 7: PCA TRANSFORMATION")
        self.log_result("=" * 80)

        # Fit PCA to capture 95% of variance
        self.X_train_pca = self.pca.fit_transform(self.X_train_scaled)
        self.X_val_pca = self.pca.transform(self.X_val_scaled)
        self.X_test_pca = self.pca.transform(self.X_test_scaled)

        self.log_result(f"PCA components: {self.pca.n_components_}")
        self.log_result(f"Variance explained: {self.pca.explained_variance_ratio_.sum():.4f}")
        self.log_result(f"Original features: {self.X_train_scaled.shape[1]} ‚Üí PCA features: {self.X_train_pca.shape[1]}")

        # Visualize PCA results
        fig, axes = plt.subplots(1, 2, figsize=(14, 5))

        # Cumulative variance helps decide how many components we need
        cumsum = np.cumsum(self.pca.explained_variance_ratio_)
        axes[0].plot(range(1, len(cumsum) + 1), cumsum, 'bo-', linewidth=2, markersize=6)
        axes[0].axhline(y=0.95, color='r', linestyle='--', linewidth=2, label='95% Threshold')
        axes[0].set_xlabel('Number of Components', fontsize=15, fontweight='bold')
        axes[0].set_ylabel('Cumulative Explained Variance', fontsize=15, fontweight='bold')
        axes[0].set_title('PCA Variance Explained', fontsize=16, fontweight='bold')
        axes[0].tick_params(axis='both', labelsize=15)
        axes[0].legend(fontsize=12, loc='best')
        axes[0].grid(True, alpha=0.3)

        # Individual component importance
        axes[1].bar(range(1, min(21, len(self.pca.explained_variance_ratio_) + 1)),
                   self.pca.explained_variance_ratio_[:20])
        axes[1].set_xlabel('Component Number', fontsize=15, fontweight='bold')
        axes[1].set_ylabel('Variance Explained', fontsize=15, fontweight='bold')
        axes[1].set_title('Top 20 PCA Components', fontsize=16, fontweight='bold')
        axes[1].tick_params(axis='both', labelsize=15)
        axes[1].grid(True, alpha=0.3, axis='y')

        plt.tight_layout()
        plt.savefig('03_pca_analysis.png', dpi=300, bbox_inches='tight')
        plt.close()
        self.log_result("\n‚úì Saved: 03_pca_analysis.png")

        return self

    def baseline_models(self):
        """
        Train simple Ridge regression as a baseline.
        This helps us understand if more complex models actually help.
        """
        self.log_result("\n" + "=" * 80)
        self.log_result("STEP 8: BASELINE MODELS (Ridge Regression)")
        self.log_result("=" * 80)

        results = []

        # Test baseline on both feature sets
        for track_name, X_tr, X_v in [
            ('Track A (Original)', self.X_train_scaled, self.X_val_scaled),
            ('Track B (PCA)', self.X_train_pca, self.X_val_pca)
        ]:
            model = Ridge(alpha=1.0, random_state=RANDOM_STATE)
            model.fit(X_tr, self.y_train)

            y_pred = model.predict(X_v)
            mse = mean_squared_error(self.y_val, y_pred)
            mae = mean_absolute_error(self.y_val, y_pred)
            r2 = r2_score(self.y_val, y_pred)

            results.append({
                'Track': track_name,
                'MAE': mae,
                'MSE': mse,
                'RMSE': np.sqrt(mse),
                'R2': r2
            })

            self.log_result(f"\n{track_name}:")
            self.log_result(f"  MAE: {mae:.4f} ‚Üê PRIMARY METRIC")
            self.log_result(f"  MSE: {mse:.4f}")
            self.log_result(f"  RMSE: {np.sqrt(mse):.4f}")
            self.log_result(f"  R¬≤: {r2:.4f}")

        self.baseline_results = pd.DataFrame(results)
        return self

    def train_multiple_models(self):
        """
        Train 9 different regression models with default parameters.
        We use 3-fold cross-validation to get robust performance estimates.
        """
        self.log_result("\n" + "=" * 80)
        self.log_result("STEP 9: TRAINING MULTIPLE MODELS (3-Fold CV)")
        self.log_result("=" * 80)

        # Dictionary of models to try - good mix of linear, tree-based, and ensemble methods
        models = {
            'Ridge': Ridge(random_state=RANDOM_STATE),
            'Lasso': Lasso(random_state=RANDOM_STATE),
            'ElasticNet': ElasticNet(random_state=RANDOM_STATE),
            'DecisionTree': DecisionTreeRegressor(random_state=RANDOM_STATE),
            'RandomForest': RandomForestRegressor(n_estimators=100, random_state=RANDOM_STATE, n_jobs=-1),
            'GradientBoosting': GradientBoostingRegressor(n_estimators=100, random_state=RANDOM_STATE),
            'XGBoost': XGBRegressor(n_estimators=100, random_state=RANDOM_STATE, n_jobs=-1),
            'AdaBoost': AdaBoostRegressor(n_estimators=100, random_state=RANDOM_STATE),
            'SVR': SVR()
        }

        results = []
        self.trained_models = {'Track A': {}, 'Track B': {}}

        # Train on both original and PCA features to see which works better
        for track_name, X_tr, X_v in [
            ('Track A', self.X_train_scaled, self.X_val_scaled),
            ('Track B', self.X_train_pca, self.X_val_pca)
        ]:
            self.log_result(f"\n{'='*60}")
            self.log_result(f"{track_name} - {'Original' if track_name == 'Track A' else 'PCA'} Features")
            self.log_result('='*60)

            for name, model in models.items():
                self.log_result(f"\nTraining {name}...")

                # Cross-validation gives us more reliable performance estimates
                cv_results = cross_validate(
                    model, X_tr, self.y_train,
                    cv=3,
                    scoring=['neg_mean_absolute_error', 'neg_mean_squared_error', 'r2'],
                    n_jobs=-1,
                    return_train_score=False
                )

                # Train on full training set for validation predictions
                model.fit(X_tr, self.y_train)

                y_pred = model.predict(X_v)
                val_mae = mean_absolute_error(self.y_val, y_pred)
                val_mse = mean_squared_error(self.y_val, y_pred)
                val_r2 = r2_score(self.y_val, y_pred)

                # Store model for later use
                self.trained_models[track_name][name] = model

                results.append({
                    'Track': track_name,
                    'Model': name,
                    'CV_MAE_mean': -cv_results['test_neg_mean_absolute_error'].mean(),
                    'CV_MAE_std': cv_results['test_neg_mean_absolute_error'].std(),
                    'CV_MSE_mean': -cv_results['test_neg_mean_squared_error'].mean(),
                    'CV_R2_mean': cv_results['test_r2'].mean(),
                    'Val_MAE': val_mae,
                    'Val_MSE': val_mse,
                    'Val_RMSE': np.sqrt(val_mse),
                    'Val_R2': val_r2
                })

                self.log_result(f"  ‚úì Val MAE: {val_mae:.4f}, R¬≤: {val_r2:.4f}")

        self.model_results = pd.DataFrame(results)

        # Display results sorted by MAE (our competition metric)
        self.log_result(f"\n{'='*80}")
        self.log_result("MODEL PERFORMANCE SUMMARY (Sorted by MAE - Lower is Better)")
        self.log_result('='*80)
        self.log_result(self.model_results.sort_values('Val_MAE')[
            ['Track', 'Model', 'Val_MAE', 'Val_MSE', 'Val_R2']
        ].to_string(index=False))

        # Create visualization comparing both tracks
        fig, axes = plt.subplots(1, 2, figsize=(16, 6))

        track_a = self.model_results[self.model_results['Track'] == 'Track A'].sort_values('Val_MAE')
        track_b = self.model_results[self.model_results['Track'] == 'Track B'].sort_values('Val_MAE')

        x_a = range(len(track_a))
        x_b = range(len(track_b))

        axes[0].barh(x_a, track_a['Val_MAE'], color='steelblue', alpha=0.8)
        axes[0].set_yticks(x_a)
        axes[0].set_yticklabels(track_a['Model'], fontsize=12, fontweight='bold')
        axes[0].set_xlabel('Validation MAE', fontsize=15, fontweight='bold')
        axes[0].set_title('Track A: Original Features', fontsize=16, fontweight='bold')
        axes[0].tick_params(axis='x', labelsize=15)
        axes[0].grid(True, alpha=0.3, axis='x')
        axes[0].invert_yaxis()

        axes[1].barh(x_b, track_b['Val_MAE'], color='coral', alpha=0.8)
        axes[1].set_yticks(x_b)
        axes[1].set_yticklabels(track_b['Model'], fontsize=12, fontweight='bold')
        axes[1].set_xlabel('Validation MAE', fontsize=15, fontweight='bold')
        axes[1].set_title('Track B: PCA Features', fontsize=16, fontweight='bold')
        axes[1].tick_params(axis='x', labelsize=15)
        axes[1].grid(True, alpha=0.3, axis='x')
        axes[1].invert_yaxis()

        plt.tight_layout()
        plt.savefig('04_model_comparison.png', dpi=300, bbox_inches='tight')
        plt.close()
        self.log_result("\n‚úì Saved: 04_model_comparison.png")

        return self

    def select_top_models(self):
        """
        Select the top 4 models from each track based on validation MAE.
        These will be tuned in the next step.
        """
        self.log_result("\n" + "=" * 80)
        self.log_result("STEP 10: SELECTING TOP 4 MODELS PER TRACK (Based on MAE)")
        self.log_result("=" * 80)

        self.top_models = {}

        for track in ['Track A', 'Track B']:
            track_results = self.model_results[self.model_results['Track'] == track]
            top_4 = track_results.nsmallest(4, 'Val_MAE')
            self.top_models[track] = top_4['Model'].tolist()

            self.log_result(f"\n{track}:")
            for i, (idx, row) in enumerate(top_4.iterrows(), 1):
                self.log_result(f"  {i}. {row['Model']}: MAE={row['Val_MAE']:.4f}, R¬≤={row['Val_R2']:.4f}")

        return self

    def hyperparameter_tuning(self):
        """
        Fine-tune hyperparameters for the top 4 models from each track.
        We use RandomizedSearchCV which is faster than GridSearch but still effective.
        """
        self.log_result("\n" + "=" * 80)
        self.log_result("STEP 11: HYPERPARAMETER TUNING (Based on MAE)")
        self.log_result("=" * 80)

        # Define search spaces for each model type
        # These ranges are based on common best practices
        param_grids = {
            'Ridge': {
                'alpha': [0.01, 0.1, 1, 10, 100]  # Regularization strength
            },
            'Lasso': {
                'alpha': [0.001, 0.01, 0.1, 1, 10]
            },
            'ElasticNet': {
                'alpha': [0.001, 0.01, 0.1, 1],
                'l1_ratio': [0.2, 0.5, 0.8]  # Balance between L1 and L2
            },
            'DecisionTree': {
                'max_depth': [5, 10, 15, 20, None],
                'min_samples_split': [2, 5, 10],
                'min_samples_leaf': [1, 2, 4]
            },
            'RandomForest': {
                'n_estimators': [100, 200, 300],
                'max_depth': [10, 20, 30, None],
                'min_samples_split': [2, 5],
                'min_samples_leaf': [1, 2]
            },
            'GradientBoosting': {
                'n_estimators': [100, 200, 300],
                'learning_rate': [0.01, 0.05, 0.1],
                'max_depth': [3, 5, 7],
                'subsample': [0.8, 1.0]
            },
            'XGBoost': {
                'n_estimators': [100, 200, 300],
                'learning_rate': [0.01, 0.05, 0.1],
                'max_depth': [3, 5, 7],
                'subsample': [0.8, 1.0],
                'colsample_bytree': [0.8, 1.0]
            },
            'AdaBoost': {
                'n_estimators': [50, 100, 200],
                'learning_rate': [0.01, 0.1, 0.5, 1.0]
            },
            'SVR': {
                'C': [0.1, 1, 10, 100],
                'epsilon': [0.01, 0.1, 0.2],
                'kernel': ['rbf', 'linear']
            }
        }

        self.tuned_models = {'Track A': {}, 'Track B': {}}
        self.best_params = {'Track A': {}, 'Track B': {}}
        tuned_results = []

        for track_name, X_tr, X_v in [
            ('Track A', self.X_train_scaled, self.X_val_scaled),
            ('Track B', self.X_train_pca, self.X_val_pca)
        ]:
            self.log_result(f"\n{track_name}:")

            for model_name in self.top_models[track_name]:
                self.log_result(f"  Tuning {model_name}...")

                base_model = self.trained_models[track_name][model_name]

                if model_name in param_grids:
                    # Randomized search tries 20 random combinations
                    search = RandomizedSearchCV(
                        base_model,
                        param_grids[model_name],
                        n_iter=20,
                        cv=3,
                        scoring='neg_mean_absolute_error',  # Optimize for MAE
                        random_state=RANDOM_STATE,
                        n_jobs=-1
                    )
                    search.fit(X_tr, self.y_train)
                    best_model = search.best_estimator_
                    best_params = search.best_params_
                    self.log_result(f"    ‚úì Best params: {best_params}")
                    self.best_params[track_name][model_name] = best_params
                else:
                    # Some models don't have tunable parameters
                    best_model = base_model
                    self.log_result("    ‚úì (no tuning)")
                    self.best_params[track_name][model_name] = "No tuning performed"

                # Evaluate tuned model
                y_pred = best_model.predict(X_v)
                mae = mean_absolute_error(self.y_val, y_pred)
                mse = mean_squared_error(self.y_val, y_pred)
                r2 = r2_score(self.y_val, y_pred)

                self.tuned_models[track_name][model_name] = best_model

                tuned_results.append({
                    'Track': track_name,
                    'Model': model_name,
                    'Val_MAE': mae,
                    'Val_MSE': mse,
                    'Val_RMSE': np.sqrt(mse),
                    'Val_R2': r2
                })

        self.tuned_results = pd.DataFrame(tuned_results)

        self.log_result("\n" + "="*80)
        self.log_result("TUNED MODEL PERFORMANCE (Sorted by MAE)")
        self.log_result("="*80)
        self.log_result(self.tuned_results.sort_values('Val_MAE').to_string(index=False))

        # Log all best hyperparameters for documentation
        self.log_result("\n" + "="*80)
        self.log_result("BEST HYPERPARAMETERS FOR ALL MODELS")
        self.log_result("="*80)
        for track in ['Track A', 'Track B']:
            self.log_result(f"\n{track}:")
            for model_name, params in self.best_params[track].items():
                self.log_result(f"  {model_name}: {params}")

        return self

    def create_ensembles(self):
        """
        Create ensemble models that combine multiple base models.
        Ensembles often outperform individual models by reducing variance.
        """
        self.log_result("\n" + "=" * 80)
        self.log_result("STEP 12: ENSEMBLE METHODS")
        self.log_result("=" * 80)

        ensemble_results = []
        self.ensemble_models = {}

        for track_name, X_tr, X_v in [
            ('Track A', self.X_train_scaled, self.X_val_scaled),
            ('Track B', self.X_train_pca, self.X_val_pca)
        ]:
            self.log_result(f"\n{track_name}:")

            # Get the top 4 tuned models for this track
            estimators = [
                (name, self.tuned_models[track_name][name])
                for name in self.top_models[track_name]
            ]

            # Voting Regressor: averages predictions from all models
            self.log_result("  Creating Voting Ensemble (equal weights)...")
            voting = VotingRegressor(estimators=estimators)
            voting.fit(X_tr, self.y_train)

            y_pred = voting.predict(X_v)
            mae = mean_absolute_error(self.y_val, y_pred)
            mse = mean_squared_error(self.y_val, y_pred)
            r2 = r2_score(self.y_val, y_pred)

            self.ensemble_models[f'{track_name}_Voting'] = voting

            ensemble_results.append({
                'Track': track_name,
                'Ensemble': 'Voting (Equal)',
                'Val_MAE': mae,
                'Val_MSE': mse,
                'Val_RMSE': np.sqrt(mse),
                'Val_R2': r2
            })
            self.log_result(f"    ‚úì MAE: {mae:.4f}")

            # Stacking Regressor: trains a meta-model on base model predictions
            self.log_result("  Creating Stacking Ensemble...")
            stacking = StackingRegressor(
                estimators=estimators,
                final_estimator=Ridge(alpha=1.0),  # Meta-learner
                cv=3  # Use CV to generate predictions for meta-learner
            )
            stacking.fit(X_tr, self.y_train)

            y_pred = stacking.predict(X_v)
            mae = mean_absolute_error(self.y_val, y_pred)
            mse = mean_squared_error(self.y_val, y_pred)
            r2 = r2_score(self.y_val, y_pred)

            self.ensemble_models[f'{track_name}_Stacking'] = stacking

            ensemble_results.append({
                'Track': track_name,
                'Ensemble': 'Stacking (Ridge)',
                'Val_MAE': mae,
                'Val_MSE': mse,
                'Val_RMSE': np.sqrt(mse),
                'Val_R2': r2
            })
            self.log_result(f"    ‚úì MAE: {mae:.4f}")

        self.ensemble_results = pd.DataFrame(ensemble_results)

        self.log_result("\n" + "="*80)
        self.log_result("ENSEMBLE PERFORMANCE (Sorted by MAE)")
        self.log_result("="*80)
        self.log_result(self.ensemble_results.sort_values('Val_MAE').to_string(index=False))

        return self

    def select_best_model(self):
        """
        Select the single best model based on validation MAE.
        This is the model we'll use for final predictions.
        """
        self.log_result("\n" + "=" * 80)
        self.log_result("STEP 13: FINAL MODEL SELECTION (Based on MAE)")
        self.log_result("=" * 80)

        # Combine results from individual models and ensembles
        all_results = pd.concat([
            self.tuned_results[['Track', 'Model', 'Val_MAE', 'Val_MSE', 'Val_R2']].rename(columns={'Model': 'Name'}),
            self.ensemble_results[['Track', 'Ensemble', 'Val_MAE', 'Val_MSE', 'Val_R2']].rename(columns={'Ensemble': 'Name'})
        ], ignore_index=True)

        # Find the model with lowest MAE
        best_idx = all_results['Val_MAE'].idxmin()
        best = all_results.loc[best_idx]

        self.log_result(f"\nüèÜ BEST MODEL: {best['Name']} ({best['Track']})")
        self.log_result(f"  Validation MAE: {best['Val_MAE']:.4f} ‚Üê PRIMARY METRIC")
        self.log_result(f"  Validation MSE: {best['Val_MSE']:.4f}")
        self.log_result(f"  Validation R¬≤: {best['Val_R2']:.4f}")

        self.best_model_name = best['Name']
        self.best_model_track = best['Track']

        # Retrieve the actual model object
        if 'Voting' in best['Name'] or 'Stacking' in best['Name']:
            self.best_model = self.ensemble_models[f"{best['Track']}_{best['Name'].split()[0]}"]
        else:
            self.best_model = self.tuned_models[best['Track']][best['Name']]

        # Determine which features to use for final predictions
        self.use_pca = (best['Track'] == 'Track B')

        self.log_result(f"\nTop 10 Models (Sorted by MAE):")
        self.log_result(all_results.nsmallest(10, 'Val_MAE')[['Track', 'Name', 'Val_MAE', 'Val_R2']].to_string(index=False))

        # Create visualization comparing top models
        fig, axes = plt.subplots(1, 2, figsize=(16, 6))

        top_10 = all_results.nsmallest(10, 'Val_MAE')

        # Color the winner gold, second silver, third bronze
        colors = ['gold' if i == 0 else 'silver' if i == 1 else 'chocolate' if i == 2 else 'steelblue'
                  for i in range(len(top_10))]

        # MAE comparison
        axes[0].barh(range(len(top_10)), top_10['Val_MAE'], color=colors, alpha=0.8)
        axes[0].set_yticks(range(len(top_10)))
        axes[0].set_yticklabels([f"{row['Name']} ({row['Track']})"
                                  for _, row in top_10.iterrows()], fontsize=11, fontweight='bold')
        axes[0].set_xlabel('Validation MAE', fontsize=15, fontweight='bold')
        axes[0].set_title('Top 10 Models - MAE Comparison', fontsize=16, fontweight='bold')
        axes[0].tick_params(axis='x', labelsize=15)
        axes[0].grid(True, alpha=0.3, axis='x')
        axes[0].invert_yaxis()

        # R¬≤ comparison
        axes[1].barh(range(len(top_10)), top_10['Val_R2'], color=colors, alpha=0.8)
        axes[1].set_yticks(range(len(top_10)))
        axes[1].set_yticklabels([f"{row['Name']} ({row['Track']})"
                                  for _, row in top_10.iterrows()], fontsize=11, fontweight='bold')
        axes[1].set_xlabel('Validation R¬≤', fontsize=15, fontweight='bold')
        axes[1].set_title('Top 10 Models - R¬≤ Comparison', fontsize=16, fontweight='bold')
        axes[1].tick_params(axis='x', labelsize=15)
        axes[1].grid(True, alpha=0.3, axis='x')
        axes[1].invert_yaxis()

        plt.tight_layout()
        plt.savefig('05_top_models_comparison.png', dpi=300, bbox_inches='tight')
        plt.close()
        self.log_result("\n‚úì Saved: 05_top_models_comparison.png")

        return self

    def retrain_and_predict(self):
        """
        Retrain the best model on the full training dataset (train + validation).
        Then generate predictions for the test set.
        """
        self.log_result("\n" + "=" * 80)
        self.log_result("STEP 14: RETRAIN ON FULL TRAINING SET")
        self.log_result("=" * 80)

        # Combine train and validation sets for final training
        if self.use_pca:
            X_full = np.vstack([self.X_train_pca, self.X_val_pca])
            X_test_final = self.X_test_pca
        else:
            X_full = np.vstack([self.X_train_scaled, self.X_val_scaled])
            X_test_final = self.X_test_scaled

        y_full = np.concatenate([self.y_train, self.y_val])

        self.log_result(f"Retraining {self.best_model_name} on full dataset...")
        self.log_result(f"  Training samples: {len(y_full)}")
        self.log_result(f"  Features: {X_full.shape[1]} ({'PCA' if self.use_pca else 'Original'})")

        # Retrain best model on all available data
        self.best_model.fit(X_full, y_full)

        self.log_result("\n" + "=" * 80)
        self.log_result("STEP 15: GENERATING PREDICTIONS")
        self.log_result("=" * 80)

        # Generate predictions for test set
        predictions = self.best_model.predict(X_test_final)

        self.log_result(f"Predictions generated for {len(predictions)} test samples")
        self.log_result(f"\nPrediction statistics:")
        self.log_result(f"  Mean: {predictions.mean():.2f}")
        self.log_result(f"  Median: {np.median(predictions):.2f}")
        self.log_result(f"  Std: {predictions.std():.2f}")
        self.log_result(f"  Min: {predictions.min():.2f}")
        self.log_result(f"  Max: {predictions.max():.2f}")

        # Create submission file
        submission = pd.DataFrame({
            'id': self.test_ids,
            'Tm': predictions
        })

        submission.to_csv('submission.csv', index=False)
        self.log_result("\n‚úì Submission file saved as 'submission.csv'")

        # Visualize predictions vs training distribution
        fig, axes = plt.subplots(1, 2, figsize=(14, 6))

        # Compare distributions
        axes[0].hist(y_full, bins=50, alpha=0.6, label='Training Data', edgecolor='black')
        axes[0].hist(predictions, bins=50, alpha=0.6, label='Test Predictions', edgecolor='black')
        axes[0].set_xlabel('Melting Point (Tm) [K]', fontsize=15, fontweight='bold')
        axes[0].set_ylabel('Frequency', fontsize=15, fontweight='bold')
        axes[0].set_title('Training vs Prediction Distribution', fontsize=16, fontweight='bold')
        axes[0].tick_params(axis='both', labelsize=15)
        axes[0].legend(fontsize=12, loc='best')
        axes[0].grid(True, alpha=0.3)

        # Actual vs Predicted scatter plot on validation set
        if self.use_pca:
            val_pred = self.best_model.predict(self.X_val_pca)
        else:
            val_pred = self.best_model.predict(self.X_val_scaled)

        axes[1].scatter(self.y_val, val_pred, alpha=0.5, s=30, edgecolors='k', linewidths=0.5)
        axes[1].plot([self.y_val.min(), self.y_val.max()],
                    [self.y_val.min(), self.y_val.max()],
                    'r--', lw=3, label='Perfect Prediction')
        axes[1].set_xlabel('Actual Tm [K]', fontsize=15, fontweight='bold')
        axes[1].set_ylabel('Predicted Tm [K]', fontsize=15, fontweight='bold')
        axes[1].set_title('Validation: Actual vs Predicted', fontsize=16, fontweight='bold')
        axes[1].tick_params(axis='both', labelsize=15)
        axes[1].legend(fontsize=12, loc='best')
        axes[1].grid(True, alpha=0.3)

        plt.tight_layout()
        plt.savefig('06_predictions_analysis.png', dpi=300, bbox_inches='tight')
        plt.close()
        self.log_result("\n‚úì Saved: 06_predictions_analysis.png")

        # Residual analysis
        residuals = self.y_val - val_pred

        fig, axes = plt.subplots(1, 2, figsize=(14, 6))

        # Residuals vs predicted - should be randomly scattered
        axes[0].scatter(val_pred, residuals, alpha=0.5, s=30, edgecolors='k', linewidths=0.5)
        axes[0].axhline(y=0, color='r', linestyle='--', linewidth=2)
        axes[0].set_xlabel('Predicted Tm [K]', fontsize=15, fontweight='bold')
        axes[0].set_ylabel('Residuals [K]', fontsize=15, fontweight='bold')
        axes[0].set_title('Residual Plot', fontsize=16, fontweight='bold')
        axes[0].tick_params(axis='both', labelsize=15)
        axes[0].grid(True, alpha=0.3)

        # Residual distribution - should be roughly normal
        axes[1].hist(residuals, bins=50, edgecolor='black', alpha=0.7)
        axes[1].axvline(x=0, color='r', linestyle='--', linewidth=2)
        axes[1].set_xlabel('Residuals [K]', fontsize=15, fontweight='bold')
        axes[1].set_ylabel('Frequency', fontsize=15, fontweight='bold')
        axes[1].set_title('Residual Distribution', fontsize=16, fontweight='bold')
        axes[1].tick_params(axis='both', labelsize=15)
        axes[1].grid(True, alpha=0.3)

        plt.tight_layout()
        plt.savefig('07_residual_analysis.png', dpi=300, bbox_inches='tight')
        plt.close()
        self.log_result("\n‚úì Saved: 07_residual_analysis.png")

        return submission

    def create_summary_visualizations(self):
        """Create comprehensive summary visualizations for the entire pipeline."""
        self.log_result("\n" + "=" * 80)
        self.log_result("CREATING SUMMARY VISUALIZATIONS")
        self.log_result("=" * 80)

        # Create a 4-panel summary figure
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))

        # Panel 1: Track comparison (MAE)
        track_comparison = self.model_results.groupby('Track')['Val_MAE'].agg(['mean', 'min', 'max'])
        x_pos = np.arange(len(track_comparison))
        axes[0, 0].bar(x_pos, track_comparison['mean'], color=['steelblue', 'coral'], alpha=0.7,
                      label='Mean MAE')
        axes[0, 0].errorbar(x_pos, track_comparison['mean'],
                           yerr=[track_comparison['mean'] - track_comparison['min'],
                                track_comparison['max'] - track_comparison['mean']],
                           fmt='none', ecolor='black', capsize=5, linewidth=2)
        axes[0, 0].set_xticks(x_pos)
        axes[0, 0].set_xticklabels(track_comparison.index, fontsize=15, fontweight='bold')
        axes[0, 0].set_ylabel('MAE', fontsize=15, fontweight='bold')
        axes[0, 0].set_title('Track Comparison: MAE', fontsize=16, fontweight='bold')
        axes[0, 0].tick_params(axis='y', labelsize=15)
        axes[0, 0].legend(fontsize=12, loc='best')
        axes[0, 0].grid(True, alpha=0.3, axis='y')

        # Panel 2: Track comparison (R¬≤)
        track_r2 = self.model_results.groupby('Track')['Val_R2'].agg(['mean', 'min', 'max'])
        axes[0, 1].bar(x_pos, track_r2['mean'], color=['steelblue', 'coral'], alpha=0.7,
                      label='Mean R¬≤')
        axes[0, 1].errorbar(x_pos, track_r2['mean'],
                           yerr=[track_r2['mean'] - track_r2['min'],
                                track_r2['max'] - track_r2['mean']],
                           fmt='none', ecolor='black', capsize=5, linewidth=2)
        axes[0, 1].set_xticks(x_pos)
        axes[0, 1].set_xticklabels(track_r2.index, fontsize=15, fontweight='bold')
        axes[0, 1].set_ylabel('R¬≤', fontsize=15, fontweight='bold')
        axes[0, 1].set_title('Track Comparison: R¬≤', fontsize=16, fontweight='bold')
        axes[0, 1].tick_params(axis='y', labelsize=15)
        axes[0, 1].legend(fontsize=12, loc='best')
        axes[0, 1].grid(True, alpha=0.3, axis='y')

        # Panel 3: Model family performance
        model_families = {
            'Linear': ['Ridge', 'Lasso', 'ElasticNet'],
            'Tree': ['DecisionTree', 'RandomForest', 'GradientBoosting', 'XGBoost', 'AdaBoost'],
            'Other': ['SVR']
        }

        family_performance = []
        for family, models in model_families.items():
            family_mae = self.model_results[self.model_results['Model'].isin(models)]['Val_MAE'].mean()
            family_performance.append({'Family': family, 'MAE': family_mae})

        family_df = pd.DataFrame(family_performance)
        axes[1, 0].bar(family_df['Family'], family_df['MAE'],
                      color=['#1f77b4', '#ff7f0e', '#2ca02c'], alpha=0.7)
        axes[1, 0].set_xlabel('Model Family', fontsize=15, fontweight='bold')
        axes[1, 0].set_ylabel('Average MAE', fontsize=15, fontweight='bold')
        axes[1, 0].set_title('Model Family Performance', fontsize=16, fontweight='bold')
        axes[1, 0].tick_params(axis='both', labelsize=15)
        axes[1, 0].grid(True, alpha=0.3, axis='y')

        # Panel 4: Default vs Tuned performance
        default_tuned = []
        for track in ['Track A', 'Track B']:
            for model in self.top_models[track]:
                default_mae = self.model_results[
                    (self.model_results['Track'] == track) &
                    (self.model_results['Model'] == model)
                ]['Val_MAE'].values[0]

                tuned_mae = self.tuned_results[
                    (self.tuned_results['Track'] == track) &
                    (self.tuned_results['Model'] == model)
                ]['Val_MAE'].values[0]

                default_tuned.append({
                    'Model': f"{model}\n({track})",
                    'Default': default_mae,
                    'Tuned': tuned_mae
                })

        dt_df = pd.DataFrame(default_tuned)
        x_dt = np.arange(len(dt_df))
        width = 0.35

        axes[1, 1].bar(x_dt - width/2, dt_df['Default'], width, label='Default', alpha=0.7)
        axes[1, 1].bar(x_dt + width/2, dt_df['Tuned'], width, label='Tuned', alpha=0.7)
        axes[1, 1].set_xticks(x_dt)
        axes[1, 1].set_xticklabels(dt_df['Model'], fontsize=10, fontweight='bold', rotation=45, ha='right')
        axes[1, 1].set_ylabel('MAE', fontsize=15, fontweight='bold')
        axes[1, 1].set_title('Default vs Tuned Models', fontsize=16, fontweight='bold')
        axes[1, 1].tick_params(axis='y', labelsize=15)
        axes[1, 1].legend(fontsize=12, loc='best')
        axes[1, 1].grid(True, alpha=0.3, axis='y')

        plt.tight_layout()
        plt.savefig('08_comprehensive_summary.png', dpi=300, bbox_inches='tight')
        plt.close()
        self.log_result("\n‚úì Saved: 08_comprehensive_summary.png")

    def run_complete_pipeline(self):
        """
        Execute the complete pipeline from start to finish.
        This is the main entry point that runs all steps in order.
        """
        self.load_data()
        self.exploratory_analysis()
        self.prepare_data()
        self.impute_missing()
        self.feature_selection()
        self.scale_features()
        self.create_pca_features()
        self.baseline_models()
        self.train_multiple_models()
        self.select_top_models()
        self.hyperparameter_tuning()
        self.create_ensembles()
        self.select_best_model()
        submission = self.retrain_and_predict()
        self.create_summary_visualizations()

        self.log_result("\n" + "="*80)
        self.log_result("PIPELINE COMPLETED SUCCESSFULLY!")
        self.log_result("="*80)
        self.log_result("\nGenerated Files:")
        self.log_result("  1. submission.csv - Final predictions")
        self.log_result("  2. model_results.txt - Complete results log")
        self.log_result("  3. 01_target_distribution.png")
        self.log_result("  4. 02_feature_importance.png")
        self.log_result("  5. 03_pca_analysis.png")
        self.log_result("  6. 04_model_comparison.png")
        self.log_result("  7. 05_top_models_comparison.png")
        self.log_result("  8. 06_predictions_analysis.png")
        self.log_result("  9. 07_residual_analysis.png")
        self.log_result("  10. 08_comprehensive_summary.png")

        # Save all results to file
        self.save_results_to_file()

        return submission


# ============================================================================
# MAIN EXECUTION
# ============================================================================

if __name__ == "__main__":
    """
    Main execution block - runs when script is executed directly.
    Simply creates a pipeline instance and runs it.
    """
    # Initialize and run the complete pipeline
    pipeline = MeltingPointPipeline()
    submission = pipeline.run_complete_pipeline()

    # Display preview of submission file
    print("\n" + "="*80)
    print("Submission preview:")
    print("="*80)
    print(submission.head(10))